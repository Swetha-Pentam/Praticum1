{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DC Assigment - Webscraping YouTube Kishore Kumar Videosusi selenium-py\n",
    "Group Members:\n",
    "Sudhavani Manne - 11810067\n",
    "Swetha Pentam - 11810068"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 : Import the necessary libraries as mentioned below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "#from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#import time\n",
    "#import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "#from math import ceil\n",
    "from selenium.webdriver.common.keys import Keys "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 : Open a CSV file for writing the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_file = open('ModiBlogs.csv', 'w', encoding=\"UTF-8\", newline=\"\")\n",
    "writer = csv.writer(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 : Now we have to create the variables and list of necessary urls which need to be scraped.\n",
    "writing the header row now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Added the attributes which was asked in the assignment\n",
    "writer.writerow([ 'link_title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 : Invoke the webdriver, exe file stored locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#driver = webdriver.Chrome('E:/Swetha/Data Science/ISB/Residency1/Data Collection/chromedriver.exe') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentding the webaddress to webdriver to open as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #get URL's page source\n",
    "#assert \"youtube\" in driver.title   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identified #search as css selector and sending the search key to webdriver youtube site for searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-b5389d477018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://timesofindia.indiatimes.com/blogs/?s=modi\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mModi_TOI_pages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://timesofindia.indiatimes.com/blogs/page/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/?s=modi\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome('E:/Swetha/Data Science/ISB/Residency1/Data Collection/chromedriver.exe')\n",
    "driver.get(\"https://timesofindia.indiatimes.com/blogs/?s=modi\") \n",
    "n=2\n",
    "Modi_TOI_pages = \"https://timesofindia.indiatimes.com/blogs/page/\" + str(n) + \"/?s=modi\"\n",
    "while (n < 200):\n",
    "    print(n)\n",
    "    reviews = driver.find_element_by_class_name(\".a\")\n",
    "    for review in reviews:\n",
    "                    review_dict = {}\n",
    "                    try:\n",
    "                        a = review.find_elements_by_class_name('title').text\n",
    "                        print(a)\n",
    "                        review_dict['link_title'] = a\n",
    "                    except:\n",
    "                        pass\n",
    "    driver.get(Modi_TOI_pages)\n",
    "    n += 1\n",
    "writer.writerow(review_dict.values())\n",
    "\n",
    "#https://timesofindia.indiatimes.com/blogs/page/2/?s=modi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-87bcdce08593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "n=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identified the Video-title is the id and fins element to scrap the web page for required fieleds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list1 = driver.find_element_by_id('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following is the code to extract link, label and title for scrapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links_list = [item.get_attribute('href') for item in list1]\n",
    "#label_list = [item.get_attribute('aria-label') for item in list1]\n",
    "title_list = [item.get_attribute('title') for item in list1]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following code splits using 'by'. Arial-lable has more wide details, hence details are scrapped from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_list1 = [item1.split('by')[1] for item1 in label_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to split the data from title list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "subscription_list = [re.split(r' [0-9]', item2,maxsplit=1)[0] for item2 in title_list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = [re.split(r' [0-9]', item2,maxsplit=1)[1] for item2 in title_list1]\n",
    "temp1 = [re.split(r' views', item2,maxsplit=1)[0] for item2 in temp]\n",
    "vedio_views_count = [re.split(' ', item2,maxsplit=0)[-1] for item2 in temp1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following is the code to get vedio uploaded date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = [word.replace('minutes ','minutes#') for word in title_list1]\n",
    "count1 = [word.replace('hours ', 'hours#') for word in count]\n",
    "count2 = [word.replace('seconds ', 'seconds#') for word in count1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following is the code to extract uploaded time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 5 years ago 1 hour, 2 minutes ', ' 2 years ago 46 minutes ', ' 1 year ago 7 hours ', ' 1 year ago 59 minutes ', ' 2 years ago 1 hour, 7 minutes ', ' 6 years ago 4 minutes, 17 seconds ', ' 2 years ago 3 minutes, 55 seconds ', ' 1 year ago 53 minutes ', ' 9 years ago 5 minutes, 15 seconds ', ' 3 years ago 5 minutes, 18 seconds ', ' 9 years ago 4 minutes, 22 seconds ', ' 3 years ago 7 minutes, 17 seconds ', ' 4 years ago 46 minutes ', ' 1 year ago 48 minutes ', ' 1 year ago 1 hour, 37 minutes ', ' 1 year ago 6 minutes, 4 seconds ', ' 10 years ago 6 minutes, 36 seconds ', ' 1 year ago 48 minutes ', ' 5 years ago 32 minutes ', ' 4 years ago 4 minutes, 33 seconds ']\n"
     ]
    }
   ],
   "source": [
    "length = len(title_list1)\n",
    "i=0  \n",
    "vedio_uploaded_dt=[]\n",
    "while (i < 20):\n",
    "    string = title_list1 [i]\n",
    "    \n",
    "    rpl1 = subscription_list [i]\n",
    "    rpl2 = vedio_views_count[i]\n",
    "    \n",
    "    time1 = re.split(rpl1,string,maxsplit=1)[1]   \n",
    "    time3 = re.split(rpl2,time1,maxsplit=1)[0]\n",
    "    vedio_uploaded_dt.append(time3)\n",
    "    i=i+1\n",
    "print(vedio_uploaded_dt)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending all the data into data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "py_df = pd.DataFrame(\n",
    "{'link tiNn    tle': title_list,\n",
    "'link url': links_list,\n",
    "'label': title_list,\n",
    " 'subscription_channel':subscription_list,\n",
    " 'number_of_views':vedio_views_count,\n",
    " 'video_upload_time' : vedio_uploaded_dt})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link over the top 10 links from the search results and verify working fine or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/watch?v=b_iSFNJmAhU\n",
      "https://www.youtube.com/watch?v=GSmXU2Q7TU8\n",
      "https://www.youtube.com/watch?v=uA67M0Lihz0\n",
      "https://www.youtube.com/watch?v=aqBHgUkoZCE\n",
      "https://www.youtube.com/watch?v=duee3ROzuKg\n",
      "https://www.youtube.com/watch?v=6vCbtPN9skk\n",
      "https://www.youtube.com/watch?v=nEnLt3pasxE\n",
      "https://www.youtube.com/watch?v=lKVIElw2IZM\n",
      "https://www.youtube.com/watch?v=AMuRRXCuy-4\n",
      "https://www.youtube.com/watch?v=MDXFi3avqo0\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for page in links_list:\n",
    "    driver.get(page)\n",
    "    time.sleep(5)\n",
    "    print(driver.current_url)\n",
    "    cnt = cnt + 1\n",
    "    if (cnt==10):\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scraping https://www.youtube.com/watch?v=b_iSFNJmAhU\n",
      "review count is taken\n",
      "review are\n",
      "name 'review' is not defined\n",
      "========================================\n",
      "Scraping https://www.youtube.com/watch?v=GSmXU2Q7TU8\n",
      "review count is taken\n",
      "review are\n",
      "name 'review' is not defined\n",
      "========================================\n",
      "Scraping https://www.youtube.com/watch?v=uA67M0Lihz0\n",
      "review count is taken\n",
      "review are\n",
      "name 'review' is not defined\n",
      "========================================\n",
      "Scraping https://www.youtube.com/watch?v=aqBHgUkoZCE\n",
      "review count is taken\n",
      "review are\n",
      "name 'review' is not defined\n",
      "========================================\n",
      "Scraping https://www.youtube.com/watch?v=duee3ROzuKg\n",
      "review count is taken\n",
      "review are\n",
      "name 'review' is not defined\n",
      "========================================\n",
      "Scraping https://www.youtube.com/watch?v=6vCbtPN9skk\n",
      "review count is taken\n",
      "review are\n",
      "name 'review' is not defined\n",
      "========================================\n",
      "Scraping https://www.youtube.com/watch?v=nEnLt3pasxE\n",
      "review count is taken\n",
      "review are\n",
      "name 'review' is not defined\n",
      "========================================\n",
      "Scraping https://www.youtube.com/watch?v=lKVIElw2IZM\n",
      "review count is taken\n",
      "review are\n",
      "name 'review' is not defined\n",
      "========================================\n",
      "Scraping https://www.youtube.com/watch?v=AMuRRXCuy-4\n",
      "review count is taken\n",
      "review are\n",
      "name 'review' is not defined\n",
      "========================================\n",
      "Scraping https://www.youtube.com/watch?v=MDXFi3avqo0\n",
      "review count is taken\n",
      "review are\n",
      "name 'review' is not defined\n"
     ]
    }
   ],
   "source": [
    "cnt = 0 \n",
    "for page in links_list:\n",
    "    cnt = cnt + 1\n",
    "    if (cnt > 10):\n",
    "        break\n",
    "    driver.get(page)\n",
    "    time.sleep(10)\n",
    "    try:\n",
    "        print(\"=\" * 40)  # Shows in terminal when a new airline is being scraped\n",
    "        print(\"Scraping \" + page)\n",
    "        \n",
    "        reviews = driver.find_elements_by_xpath('//h2[@id = \"count\"]//span[@class = \"style-scope ytd-comment-renderer\"]')\n",
    "        print(\"review count is taken\")\n",
    "        # Iterate through all the pages of reviews for the Youtube video links\n",
    "        index = 1\n",
    "        while index <= 50:\n",
    "                     \n",
    "            time.sleep(5)\n",
    "\n",
    "            try:\n",
    "                    index = index + 1\n",
    "                    reviews = driver.find_elements_by_xpath('//div[@id = \"contents\"]')\n",
    "                    print(\"review are\")\n",
    "                    print(review)\n",
    "                    review_count = float(review_count)\n",
    "                    n = int(ceil(review_count / 10))\n",
    "# Iterate through all the comments \n",
    "# Initialize an empty dictionary for each review\n",
    "                    review_dict = {}\n",
    "                    \n",
    "                    # Find xpaths of the fields desired as columns in future data frame\n",
    "                    \n",
    "                    try:\n",
    "                        comment_user_handle = review.find_element_by_xpath(\n",
    "                            '//span[@class = \"style-scope ytd-comment-renderer\"]')\n",
    "                        print(comment_user_handle)\n",
    "                    except:\n",
    "                        comment_user_handle = \"\"\n",
    "                    print(\"comment_user_handle\")\n",
    "                    print(comment_user_handle)\n",
    "                    try:\n",
    "                        comment_posted_date = review.find_element_by_xpath('//a[@class = \"yt-simple-endpoint style-scope yt-formatted-string\"]')\n",
    "                    except:\n",
    "                        comment_posted_date = \"\"\n",
    "                    print(\"comment_posted_date\")\n",
    "                    print(comment_posted_date)\n",
    "                    try:\n",
    "                        customer_review = review.find_element_by_xpath(\n",
    "                            '//yt-formatted-string[@id = \"content-text\"]')\n",
    "                    except:\n",
    "                        customer_review = \"\"\n",
    "                    try:\n",
    "                        comment_replies_count = review.find_element_by_xpath(\n",
    "                            '//div[@id = \"replies\"]//span[@id = \"more-text\"]')\n",
    "                    except:\n",
    "                        comment_replies_count = \"\"\n",
    "                    try:\n",
    "                        comment_upvotes_count = review.find_element_by_xpath(\n",
    "                            '//ytd-toggle-button-renderer[@id = \"like-button\"]//span[@id = \"vote-count-middle\"]')\n",
    "                    except:\n",
    "                        comment_upvotes_count = \"\"\n",
    "                    comment_downvotes_count = \"\"\n",
    "                    \n",
    "                    # Write the results of the above to a dictionary. Note that each overall review will have its\n",
    "                    # own dictionary, but all dictionaries for all the rows will all have the same keys. This\n",
    "                    # allows Selenium to write the contents of these dictionaries into a coherent .csv file\n",
    "                    review_dict['link_title'] = video_title\n",
    "                    review_dict['number_of_views'] = video_duration\n",
    "                    review_dict['subscription_channel'] = subscription_channel\n",
    "                    review_dict['comment_user_handle'] = comment_user_handle\n",
    "                    review_dict['comment_posted_date'] = comment_posted_date\n",
    "                    review_dict['customer_review'] = customer_review\n",
    "                    #review_dict['review_date'] = review_date\n",
    "                    review_dict['customer_review'] = customer_review\n",
    "                    review_dict['comment_replies_count'] = comment_replies_count\n",
    "                    review_dict['review_comment_upvotes_count'] = comment_upvotes_count\n",
    "                    review_dict['review_comment_downvotes_count'] = comment_downvotes_count\n",
    "                    \n",
    "                    print(\"extracted the data\")\n",
    "                    print(video_title)\n",
    "                    print(subscription_channel)\n",
    "                    print(customer_review)\n",
    "                    print(comment_posted_date)\n",
    "                    writer.writerow(review_dict.values())\n",
    "                    \n",
    "\n",
    "            # If an error is thrown unrelated to the above variables, print the error to the terminal\n",
    "            # console, close the .csv file, and break the while loop.\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                csv_file.close()\n",
    "                # driver.close()\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        csv_file.close()\n",
    "        # driver.close()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
